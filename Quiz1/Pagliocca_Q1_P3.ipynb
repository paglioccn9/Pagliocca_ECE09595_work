{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pagliocca_Q1_P3.ipynb","provenance":[],"authorship_tag":"ABX9TyMXF5L0qnOz8ya8WubP3y7r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PeeDF8S4DeRF","executionInfo":{"status":"ok","timestamp":1604975781489,"user_tz":480,"elapsed":2532452,"user":{"displayName":"Nicholas Pagliocca","photoUrl":"","userId":"15154366603312876591"}},"outputId":"01ee87fd-341d-4074-89f8-a67ee7847150","colab":{"base_uri":"https://localhost:8080/"}},"source":["'''\n","\n","Q3 Inception Module for CIFAR-10 dataset\n","\n","'''\n","import keras\n","from keras.models import Model\n","from keras.layers import Conv2D, MaxPooling2D, Dropout\n","from keras.layers import Flatten, Dense\n","from keras.layers import Input\n","from keras.utils import np_utils\n","from keras.datasets import cifar10\n","import tensorflow as tf\n","import pandas\n","epochs = 100\n","\n","# Get the data\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","# Get the data ready\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","\n","y_train = np_utils.to_categorical(y_train)\n","y_test = np_utils.to_categorical(y_test)\n","\n","# Create imput\n","input_img = Input(shape = (32, 32, 3))\n","\n","\n","# Create Volumes for the Inception module\n","volume_1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n","\n","volume_2 = Conv2D(96, (1,1), padding='same', activation='relu')(input_img)\n","volume_2 = Conv2D(128, (3,3), padding='same', activation='relu')(volume_2)\n","\n","volume_3 = Conv2D(16, (1,1), padding='same', activation='relu')(input_img)\n","volume_3 = Conv2D(32, (5,5), padding='same', activation='relu')(volume_3)\n","\n","volume_4 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n","\n","volume_4 = Conv2D(32, (1,1), padding='same', activation='relu')(volume_4)\n","\n","volume_5 = Conv2D(128, (1,1), padding='same', activation='relu')(input_img)\n","\n","volume_6 = Conv2D(128, (1,1), padding='same', activation='relu')(input_img)\n","volume_6 = Conv2D(192, (3,3), padding='same', activation='relu')(volume_6)\n","\n","volume_7 = Conv2D(32, (1,1), padding='same', activation='relu')(input_img)\n","volume_7 = Conv2D(96, (5,5), padding='same', activation='relu')(volume_7)\n","\n","volume_8 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n","volume_8 = Conv2D(64, (1,1), padding='same', activation='relu')(volume_8)\n","\n","volume_9 = Conv2D(196, (1,1), padding='same', activation='relu')(input_img)\n","\n","volume_10 = Conv2D(96, (1,1), padding='same', activation='relu')(input_img)\n","volume_10 = Conv2D(128, (3,3), padding='same', activation='relu')(volume_9)\n","\n","volume_11 = Conv2D(16, (1,1), padding='same', activation='relu')(input_img)\n","volume_11 = Conv2D(32, (5,5), padding='same', activation='relu')(volume_10)\n","\n","volume_12 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n","volume_12 = Conv2D(32, (1,1), padding='same', activation='relu')(volume_11)\n","\n","\n","volume_13 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n","\n","volume_14 = Conv2D(96, (1,1), padding='same', activation='relu')(input_img)\n","volume_14 = Conv2D(128, (3,3), padding='same', activation='relu')(volume_13)\n","\n","volume_15 = Conv2D(16, (1,1), padding='same', activation='relu')(input_img)\n","volume_15 = Conv2D(32, (5,5), padding='same', activation='relu')(volume_14)\n","\n","volume_16 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n","volume_16 = Conv2D(32, (1,1), padding='same', activation='relu')(volume_15)\n","\n","# Concatenate all volumes of the Inception module\n","inception_module = keras.layers.concatenate([volume_1, volume_2, volume_3,\n","                                             volume_4], axis = 3)\n","inception_module1 = keras.layers.concatenate([volume_5, volume_6, volume_7,\n","                                             volume_8], axis = 3)\n","inception_module2 = keras.layers.concatenate([volume_9, volume_10, volume_11,\n","                                             volume_12], axis = 3)\n","inception_module3 = keras.layers.concatenate([volume_13, volume_14, volume_15,\n","                                             volume_16], axis = 3)\n","inception_modulea = keras.layers.concatenate([inception_module, inception_module1,inception_module2,inception_module3])\n","output = Flatten()(inception_modulea)\n","outputa = Dropout(0.5)(output)\n","out    = Dense(10, activation='softmax')(output)\n","\n","model = Model(inputs = input_img, outputs = out)\n","\n","print(model.summary())\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=512)\n","\n","pandas.DataFrame(history.history).to_csv(\"historynew2.csv\")\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"functional_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n","__________________________________________________________________________________________________\n","conv2d_61 (Conv2D)              (None, 32, 32, 196)  784         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_67 (Conv2D)              (None, 32, 32, 64)   256         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_63 (Conv2D)              (None, 32, 32, 128)  225920      conv2d_61[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_69 (Conv2D)              (None, 32, 32, 128)  73856       conv2d_67[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 32, 32, 96)   384         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 32, 32, 16)   64          input_4[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 3)    0           input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 32, 32, 128)  512         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_58 (Conv2D)              (None, 32, 32, 32)   128         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling2d_9 (MaxPooling2D)  (None, 32, 32, 3)    0           input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_65 (Conv2D)              (None, 32, 32, 32)   102432      conv2d_63[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_71 (Conv2D)              (None, 32, 32, 32)   102432      conv2d_69[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 32, 32, 64)   256         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 32, 32, 128)  110720      conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 32, 32, 32)   12832       conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 32, 32, 32)   128         max_pooling2d_8[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 32, 32, 128)  512         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_57 (Conv2D)              (None, 32, 32, 192)  221376      conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_59 (Conv2D)              (None, 32, 32, 96)   76896       conv2d_58[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_60 (Conv2D)              (None, 32, 32, 64)   256         max_pooling2d_9[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_66 (Conv2D)              (None, 32, 32, 32)   1056        conv2d_65[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_72 (Conv2D)              (None, 32, 32, 32)   1056        conv2d_71[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 32, 32, 256)  0           conv2d_49[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_53[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_11 (Concatenate)    (None, 32, 32, 480)  0           conv2d_55[0][0]                  \n","                                                                 conv2d_57[0][0]                  \n","                                                                 conv2d_59[0][0]                  \n","                                                                 conv2d_60[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 32, 32, 388)  0           conv2d_61[0][0]                  \n","                                                                 conv2d_63[0][0]                  \n","                                                                 conv2d_65[0][0]                  \n","                                                                 conv2d_66[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_13 (Concatenate)    (None, 32, 32, 256)  0           conv2d_67[0][0]                  \n","                                                                 conv2d_69[0][0]                  \n","                                                                 conv2d_71[0][0]                  \n","                                                                 conv2d_72[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_14 (Concatenate)    (None, 32, 32, 1380) 0           concatenate_10[0][0]             \n","                                                                 concatenate_11[0][0]             \n","                                                                 concatenate_12[0][0]             \n","                                                                 concatenate_13[0][0]             \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 1413120)      0           concatenate_14[0][0]             \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           14131210    flatten_2[0][0]                  \n","==================================================================================================\n","Total params: 15,063,066\n","Trainable params: 15,063,066\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/100\n"," 2/98 [..............................] - ETA: 19s - loss: 24.0262 - accuracy: 0.1006WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1489s). Check your callbacks.\n","98/98 [==============================] - 26s 261ms/step - loss: 8.3109 - accuracy: 0.2959 - val_loss: 1.5817 - val_accuracy: 0.4571\n","Epoch 2/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.3693 - accuracy: 0.5201 - val_loss: 1.2232 - val_accuracy: 0.5682\n","Epoch 3/100\n","98/98 [==============================] - 25s 256ms/step - loss: 1.1105 - accuracy: 0.6123 - val_loss: 1.1556 - val_accuracy: 0.5902\n","Epoch 4/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.9447 - accuracy: 0.6708 - val_loss: 1.0753 - val_accuracy: 0.6251\n","Epoch 5/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.8366 - accuracy: 0.7065 - val_loss: 0.9861 - val_accuracy: 0.6604\n","Epoch 6/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.7103 - accuracy: 0.7540 - val_loss: 0.9726 - val_accuracy: 0.6712\n","Epoch 7/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.6108 - accuracy: 0.7895 - val_loss: 1.0731 - val_accuracy: 0.6600\n","Epoch 8/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.5176 - accuracy: 0.8230 - val_loss: 1.0703 - val_accuracy: 0.6667\n","Epoch 9/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.4110 - accuracy: 0.8618 - val_loss: 1.0923 - val_accuracy: 0.6659\n","Epoch 10/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.3265 - accuracy: 0.8932 - val_loss: 1.1486 - val_accuracy: 0.6725\n","Epoch 11/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.2466 - accuracy: 0.9231 - val_loss: 1.2146 - val_accuracy: 0.6740\n","Epoch 12/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.1778 - accuracy: 0.9482 - val_loss: 1.3658 - val_accuracy: 0.6673\n","Epoch 13/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.1324 - accuracy: 0.9625 - val_loss: 1.4425 - val_accuracy: 0.6733\n","Epoch 14/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0947 - accuracy: 0.9752 - val_loss: 1.5204 - val_accuracy: 0.6776\n","Epoch 15/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0684 - accuracy: 0.9839 - val_loss: 1.7231 - val_accuracy: 0.6629\n","Epoch 16/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0557 - accuracy: 0.9869 - val_loss: 1.7453 - val_accuracy: 0.6702\n","Epoch 17/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0365 - accuracy: 0.9933 - val_loss: 1.8585 - val_accuracy: 0.6746\n","Epoch 18/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0268 - accuracy: 0.9952 - val_loss: 1.9512 - val_accuracy: 0.6701\n","Epoch 19/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 2.0443 - val_accuracy: 0.6697\n","Epoch 20/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0145 - accuracy: 0.9981 - val_loss: 2.1482 - val_accuracy: 0.6611\n","Epoch 21/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0153 - accuracy: 0.9978 - val_loss: 2.2097 - val_accuracy: 0.6640\n","Epoch 22/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0319 - accuracy: 0.9915 - val_loss: 2.2730 - val_accuracy: 0.6612\n","Epoch 23/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 2.3198 - val_accuracy: 0.6521\n","Epoch 24/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0558 - accuracy: 0.9820 - val_loss: 2.3076 - val_accuracy: 0.6607\n","Epoch 25/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0231 - accuracy: 0.9944 - val_loss: 2.5962 - val_accuracy: 0.6506\n","Epoch 26/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 2.4846 - val_accuracy: 0.6648\n","Epoch 27/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 2.5581 - val_accuracy: 0.6674\n","Epoch 28/100\n","98/98 [==============================] - 25s 256ms/step - loss: 0.0081 - accuracy: 0.9988 - val_loss: 2.6012 - val_accuracy: 0.6665\n","Epoch 29/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 2.5841 - val_accuracy: 0.6696\n","Epoch 30/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 2.6450 - val_accuracy: 0.6723\n","Epoch 31/100\n","98/98 [==============================] - 25s 257ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 2.6719 - val_accuracy: 0.6757\n","Epoch 32/100\n","98/98 [==============================] - 25s 257ms/step - loss: 8.2912e-04 - accuracy: 1.0000 - val_loss: 2.7124 - val_accuracy: 0.6757\n","Epoch 33/100\n","98/98 [==============================] - 25s 256ms/step - loss: 6.8445e-04 - accuracy: 1.0000 - val_loss: 2.7492 - val_accuracy: 0.6761\n","Epoch 34/100\n","98/98 [==============================] - 25s 257ms/step - loss: 5.8755e-04 - accuracy: 1.0000 - val_loss: 2.7786 - val_accuracy: 0.6767\n","Epoch 35/100\n","98/98 [==============================] - 25s 256ms/step - loss: 5.1370e-04 - accuracy: 1.0000 - val_loss: 2.8110 - val_accuracy: 0.6762\n","Epoch 36/100\n","98/98 [==============================] - 25s 257ms/step - loss: 4.5801e-04 - accuracy: 1.0000 - val_loss: 2.8416 - val_accuracy: 0.6765\n","Epoch 37/100\n","98/98 [==============================] - 25s 256ms/step - loss: 4.1535e-04 - accuracy: 1.0000 - val_loss: 2.8719 - val_accuracy: 0.6769\n","Epoch 38/100\n","98/98 [==============================] - 25s 256ms/step - loss: 3.7381e-04 - accuracy: 1.0000 - val_loss: 2.8963 - val_accuracy: 0.6762\n","Epoch 39/100\n","98/98 [==============================] - 25s 256ms/step - loss: 3.4078e-04 - accuracy: 1.0000 - val_loss: 2.9230 - val_accuracy: 0.6762\n","Epoch 40/100\n","98/98 [==============================] - 25s 257ms/step - loss: 3.1103e-04 - accuracy: 1.0000 - val_loss: 2.9497 - val_accuracy: 0.6762\n","Epoch 41/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.8412e-04 - accuracy: 1.0000 - val_loss: 2.9724 - val_accuracy: 0.6761\n","Epoch 42/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.6119e-04 - accuracy: 1.0000 - val_loss: 2.9999 - val_accuracy: 0.6753\n","Epoch 43/100\n","98/98 [==============================] - 25s 256ms/step - loss: 2.4125e-04 - accuracy: 1.0000 - val_loss: 3.0192 - val_accuracy: 0.6767\n","Epoch 44/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.2236e-04 - accuracy: 1.0000 - val_loss: 3.0451 - val_accuracy: 0.6754\n","Epoch 45/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.0547e-04 - accuracy: 1.0000 - val_loss: 3.0641 - val_accuracy: 0.6752\n","Epoch 46/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.8952e-04 - accuracy: 1.0000 - val_loss: 3.0896 - val_accuracy: 0.6753\n","Epoch 47/100\n","98/98 [==============================] - 25s 256ms/step - loss: 1.7676e-04 - accuracy: 1.0000 - val_loss: 3.1112 - val_accuracy: 0.6751\n","Epoch 48/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.6427e-04 - accuracy: 1.0000 - val_loss: 3.1327 - val_accuracy: 0.6752\n","Epoch 49/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.5314e-04 - accuracy: 1.0000 - val_loss: 3.1539 - val_accuracy: 0.6748\n","Epoch 50/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.4273e-04 - accuracy: 1.0000 - val_loss: 3.1749 - val_accuracy: 0.6757\n","Epoch 51/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.3337e-04 - accuracy: 1.0000 - val_loss: 3.1943 - val_accuracy: 0.6744\n","Epoch 52/100\n","98/98 [==============================] - 25s 256ms/step - loss: 1.2443e-04 - accuracy: 1.0000 - val_loss: 3.2152 - val_accuracy: 0.6743\n","Epoch 53/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.1599e-04 - accuracy: 1.0000 - val_loss: 3.2370 - val_accuracy: 0.6747\n","Epoch 54/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.0857e-04 - accuracy: 1.0000 - val_loss: 3.2560 - val_accuracy: 0.6738\n","Epoch 55/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.0195e-04 - accuracy: 1.0000 - val_loss: 3.2745 - val_accuracy: 0.6750\n","Epoch 56/100\n","98/98 [==============================] - 25s 256ms/step - loss: 9.5287e-05 - accuracy: 1.0000 - val_loss: 3.2939 - val_accuracy: 0.6745\n","Epoch 57/100\n","98/98 [==============================] - 25s 257ms/step - loss: 8.9530e-05 - accuracy: 1.0000 - val_loss: 3.3138 - val_accuracy: 0.6743\n","Epoch 58/100\n","98/98 [==============================] - 25s 257ms/step - loss: 8.3914e-05 - accuracy: 1.0000 - val_loss: 3.3328 - val_accuracy: 0.6744\n","Epoch 59/100\n","98/98 [==============================] - 25s 257ms/step - loss: 7.8633e-05 - accuracy: 1.0000 - val_loss: 3.3523 - val_accuracy: 0.6747\n","Epoch 60/100\n","98/98 [==============================] - 25s 257ms/step - loss: 7.3867e-05 - accuracy: 1.0000 - val_loss: 3.3721 - val_accuracy: 0.6743\n","Epoch 61/100\n","98/98 [==============================] - 25s 257ms/step - loss: 6.9336e-05 - accuracy: 1.0000 - val_loss: 3.3918 - val_accuracy: 0.6747\n","Epoch 62/100\n","98/98 [==============================] - 25s 257ms/step - loss: 6.5131e-05 - accuracy: 1.0000 - val_loss: 3.4109 - val_accuracy: 0.6753\n","Epoch 63/100\n","98/98 [==============================] - 25s 257ms/step - loss: 6.1168e-05 - accuracy: 1.0000 - val_loss: 3.4301 - val_accuracy: 0.6744\n","Epoch 64/100\n","98/98 [==============================] - 25s 257ms/step - loss: 5.7521e-05 - accuracy: 1.0000 - val_loss: 3.4485 - val_accuracy: 0.6745\n","Epoch 65/100\n","98/98 [==============================] - 25s 257ms/step - loss: 5.4271e-05 - accuracy: 1.0000 - val_loss: 3.4679 - val_accuracy: 0.6738\n","Epoch 66/100\n","98/98 [==============================] - 25s 257ms/step - loss: 5.0941e-05 - accuracy: 1.0000 - val_loss: 3.4871 - val_accuracy: 0.6746\n","Epoch 67/100\n","98/98 [==============================] - 25s 257ms/step - loss: 4.7919e-05 - accuracy: 1.0000 - val_loss: 3.5028 - val_accuracy: 0.6743\n","Epoch 68/100\n","98/98 [==============================] - 25s 257ms/step - loss: 4.5137e-05 - accuracy: 1.0000 - val_loss: 3.5211 - val_accuracy: 0.6745\n","Epoch 69/100\n","98/98 [==============================] - 25s 257ms/step - loss: 4.2520e-05 - accuracy: 1.0000 - val_loss: 3.5401 - val_accuracy: 0.6736\n","Epoch 70/100\n","98/98 [==============================] - 25s 257ms/step - loss: 4.0027e-05 - accuracy: 1.0000 - val_loss: 3.5579 - val_accuracy: 0.6738\n","Epoch 71/100\n","98/98 [==============================] - 25s 256ms/step - loss: 3.7682e-05 - accuracy: 1.0000 - val_loss: 3.5787 - val_accuracy: 0.6744\n","Epoch 72/100\n","98/98 [==============================] - 25s 257ms/step - loss: 3.5540e-05 - accuracy: 1.0000 - val_loss: 3.5959 - val_accuracy: 0.6742\n","Epoch 73/100\n","98/98 [==============================] - 25s 257ms/step - loss: 3.3572e-05 - accuracy: 1.0000 - val_loss: 3.6154 - val_accuracy: 0.6741\n","Epoch 74/100\n","98/98 [==============================] - 25s 257ms/step - loss: 3.1515e-05 - accuracy: 1.0000 - val_loss: 3.6330 - val_accuracy: 0.6741\n","Epoch 75/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.9677e-05 - accuracy: 1.0000 - val_loss: 3.6505 - val_accuracy: 0.6741\n","Epoch 76/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.8087e-05 - accuracy: 1.0000 - val_loss: 3.6721 - val_accuracy: 0.6739\n","Epoch 77/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.6450e-05 - accuracy: 1.0000 - val_loss: 3.6890 - val_accuracy: 0.6738\n","Epoch 78/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.4944e-05 - accuracy: 1.0000 - val_loss: 3.7042 - val_accuracy: 0.6739\n","Epoch 79/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.3556e-05 - accuracy: 1.0000 - val_loss: 3.7223 - val_accuracy: 0.6740\n","Epoch 80/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.2176e-05 - accuracy: 1.0000 - val_loss: 3.7411 - val_accuracy: 0.6731\n","Epoch 81/100\n","98/98 [==============================] - 25s 257ms/step - loss: 2.0929e-05 - accuracy: 1.0000 - val_loss: 3.7604 - val_accuracy: 0.6732\n","Epoch 82/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.9736e-05 - accuracy: 1.0000 - val_loss: 3.7777 - val_accuracy: 0.6737\n","Epoch 83/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.8641e-05 - accuracy: 1.0000 - val_loss: 3.7952 - val_accuracy: 0.6731\n","Epoch 84/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.7541e-05 - accuracy: 1.0000 - val_loss: 3.8113 - val_accuracy: 0.6736\n","Epoch 85/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.6662e-05 - accuracy: 1.0000 - val_loss: 3.8314 - val_accuracy: 0.6733\n","Epoch 86/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.5647e-05 - accuracy: 1.0000 - val_loss: 3.8490 - val_accuracy: 0.6729\n","Epoch 87/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.4802e-05 - accuracy: 1.0000 - val_loss: 3.8668 - val_accuracy: 0.6728\n","Epoch 88/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.3966e-05 - accuracy: 1.0000 - val_loss: 3.8844 - val_accuracy: 0.6734\n","Epoch 89/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.3175e-05 - accuracy: 1.0000 - val_loss: 3.8996 - val_accuracy: 0.6723\n","Epoch 90/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.2432e-05 - accuracy: 1.0000 - val_loss: 3.9189 - val_accuracy: 0.6724\n","Epoch 91/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.1741e-05 - accuracy: 1.0000 - val_loss: 3.9390 - val_accuracy: 0.6732\n","Epoch 92/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.1117e-05 - accuracy: 1.0000 - val_loss: 3.9556 - val_accuracy: 0.6728\n","Epoch 93/100\n","98/98 [==============================] - 25s 257ms/step - loss: 1.0459e-05 - accuracy: 1.0000 - val_loss: 3.9733 - val_accuracy: 0.6728\n","Epoch 94/100\n","98/98 [==============================] - 25s 257ms/step - loss: 9.9096e-06 - accuracy: 1.0000 - val_loss: 3.9935 - val_accuracy: 0.6729\n","Epoch 95/100\n","98/98 [==============================] - 25s 256ms/step - loss: 9.3623e-06 - accuracy: 1.0000 - val_loss: 4.0097 - val_accuracy: 0.6724\n","Epoch 96/100\n","98/98 [==============================] - 25s 256ms/step - loss: 8.8266e-06 - accuracy: 1.0000 - val_loss: 4.0301 - val_accuracy: 0.6725\n","Epoch 97/100\n","98/98 [==============================] - 25s 258ms/step - loss: 8.3208e-06 - accuracy: 1.0000 - val_loss: 4.0448 - val_accuracy: 0.6716\n","Epoch 98/100\n","98/98 [==============================] - 25s 257ms/step - loss: 7.8552e-06 - accuracy: 1.0000 - val_loss: 4.0626 - val_accuracy: 0.6718\n","Epoch 99/100\n","98/98 [==============================] - 25s 257ms/step - loss: 7.4493e-06 - accuracy: 1.0000 - val_loss: 4.0826 - val_accuracy: 0.6722\n","Epoch 100/100\n","98/98 [==============================] - 25s 257ms/step - loss: 7.0559e-06 - accuracy: 1.0000 - val_loss: 4.1014 - val_accuracy: 0.6720\n","Accuracy: 67.20%\n"],"name":"stdout"}]}]}
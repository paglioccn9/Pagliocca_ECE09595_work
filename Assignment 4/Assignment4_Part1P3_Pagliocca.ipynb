{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment4_Part1P3_Pagliocca.ipynb","provenance":[],"authorship_tag":"ABX9TyOkwfF8jrB/LETKWC7h4N0b"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EfCcgOMj5wN_","executionInfo":{"status":"ok","timestamp":1603635125904,"user_tz":420,"elapsed":2368,"user":{"displayName":"Nicholas Pagliocca","photoUrl":"","userId":"15154366603312876591"}},"outputId":"17d0114a-c7c5-46ae-c9cd-81c89ae34181","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#This code removes the attention mechanism, which makes the model underperform and at times will not guess close to the correct phrase\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","\n","# Download the file\n","path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True)\n","\n","path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AuKGEuVK_2U-","executionInfo":{"status":"ok","timestamp":1603638478289,"user_tz":420,"elapsed":8283,"user":{"displayName":"Nicholas Pagliocca","photoUrl":"","userId":"15154366603312876591"}},"outputId":"e0458ca9-ebe6-4f8f-ff46-e03c28428cb6","colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","  w = w.strip()\n","  w = '<start> ' + w + ' <end>'\n","  return w\n","\n","en_sentence = u\"May I borrow this book?\"\n","sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n","\n","def create_dataset(path, num_examples):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","  return zip(*word_pairs)\n","\n","en, sp = create_dataset(path_to_file, None)\n","\n","def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","  return tensor, lang_tokenizer\n","\n","def load_dataset(path, num_examples=None):\n","  targ_lang, inp_lang = create_dataset(path, num_examples)\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n","\n","num_examples = 30000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n","\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n","\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n","\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","convert(targ_lang, target_tensor_train[0])\n","\n","\n","BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, x, hidden, enc_output):\n","    x = self.embedding(x)\n","    output, state = self.gru(x)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights\n","\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)\n","\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","    for t in range(1, targ.shape[1]):\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","      loss += loss_function(targ[:, t], predictions)\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 ----> <start>\n","12 ----> me\n","40 ----> gusta\n","13 ----> la\n","748 ----> luz\n","14 ----> de\n","39 ----> las\n","2242 ----> velas\n","3 ----> .\n","2 ----> <end>\n","\n","1 ----> <start>\n","4 ----> i\n","35 ----> like\n","2866 ----> candlelight\n","3 ----> .\n","2 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWJEk_eB50ND","executionInfo":{"status":"ok","timestamp":1603642363859,"user_tz":420,"elapsed":3853964,"user":{"displayName":"Nicholas Pagliocca","photoUrl":"","userId":"15154366603312876591"}},"outputId":"57871d66-2b0f-4b6e-8632-99df3b30b6b5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(inp, targ, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['encoder_32/embedding_62/embeddings:0', 'encoder_32/gru_62/gru_cell_62/kernel:0', 'encoder_32/gru_62/gru_cell_62/recurrent_kernel:0', 'encoder_32/gru_62/gru_cell_62/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['encoder_32/embedding_62/embeddings:0', 'encoder_32/gru_62/gru_cell_62/kernel:0', 'encoder_32/gru_62/gru_cell_62/recurrent_kernel:0', 'encoder_32/gru_62/gru_cell_62/bias:0'] when minimizing the loss.\n","Epoch 1 Batch 0 Loss 4.7230\n","Epoch 1 Batch 100 Loss 2.2457\n","Epoch 1 Batch 200 Loss 2.0341\n","Epoch 1 Batch 300 Loss 1.9545\n","Epoch 1 Loss 2.2523\n","Time taken for 1 epoch 398.89194679260254 sec\n","\n","Epoch 1 Loss 2.2523\n","Time taken for 1 epoch 398.89282608032227 sec\n","\n","Epoch 2 Batch 0 Loss 1.8326\n","Epoch 2 Batch 100 Loss 1.7701\n","Epoch 2 Batch 200 Loss 1.7958\n","Epoch 2 Batch 300 Loss 1.9179\n","Epoch 2 Loss 1.8167\n","Time taken for 1 epoch 382.7709789276123 sec\n","\n","Epoch 2 Loss 1.8167\n","Time taken for 1 epoch 382.7715060710907 sec\n","\n","Epoch 3 Batch 0 Loss 1.6896\n","Epoch 3 Batch 100 Loss 1.7144\n","Epoch 3 Batch 200 Loss 1.7024\n","Epoch 3 Batch 300 Loss 1.6860\n","Epoch 3 Loss 1.7296\n","Time taken for 1 epoch 383.22859716415405 sec\n","\n","Epoch 3 Loss 1.7296\n","Time taken for 1 epoch 383.2294924259186 sec\n","\n","Epoch 4 Batch 0 Loss 1.6507\n","Epoch 4 Batch 100 Loss 1.6469\n","Epoch 4 Batch 200 Loss 1.7544\n","Epoch 4 Batch 300 Loss 1.6832\n","Epoch 4 Loss 1.6728\n","Time taken for 1 epoch 382.73137283325195 sec\n","\n","Epoch 4 Loss 1.6728\n","Time taken for 1 epoch 382.7319760322571 sec\n","\n","Epoch 5 Batch 0 Loss 1.5709\n","Epoch 5 Batch 100 Loss 1.5458\n","Epoch 5 Batch 200 Loss 1.5830\n","Epoch 5 Batch 300 Loss 1.5828\n","Epoch 5 Loss 1.6335\n","Time taken for 1 epoch 382.3797106742859 sec\n","\n","Epoch 5 Loss 1.6335\n","Time taken for 1 epoch 382.3811311721802 sec\n","\n","Epoch 6 Batch 0 Loss 1.6010\n","Epoch 6 Batch 100 Loss 1.5906\n","Epoch 6 Batch 200 Loss 1.5623\n","Epoch 6 Batch 300 Loss 1.6852\n","Epoch 6 Loss 1.6052\n","Time taken for 1 epoch 383.59428000450134 sec\n","\n","Epoch 6 Loss 1.6052\n","Time taken for 1 epoch 383.59478878974915 sec\n","\n","Epoch 7 Batch 0 Loss 1.5042\n","Epoch 7 Batch 100 Loss 1.5330\n","Epoch 7 Batch 200 Loss 1.4934\n","Epoch 7 Batch 300 Loss 1.6208\n","Epoch 7 Loss 1.5846\n","Time taken for 1 epoch 384.2243621349335 sec\n","\n","Epoch 7 Loss 1.5846\n","Time taken for 1 epoch 384.22538566589355 sec\n","\n","Epoch 8 Batch 0 Loss 1.5098\n","Epoch 8 Batch 100 Loss 1.6620\n","Epoch 8 Batch 200 Loss 1.5280\n","Epoch 8 Batch 300 Loss 1.6513\n","Epoch 8 Loss 1.5690\n","Time taken for 1 epoch 384.9074137210846 sec\n","\n","Epoch 8 Loss 1.5690\n","Time taken for 1 epoch 384.9075665473938 sec\n","\n","Epoch 9 Batch 0 Loss 1.4303\n","Epoch 9 Batch 100 Loss 1.5967\n","Epoch 9 Batch 200 Loss 1.5920\n","Epoch 9 Batch 300 Loss 1.6961\n","Epoch 9 Loss 1.5576\n","Time taken for 1 epoch 387.4278392791748 sec\n","\n","Epoch 9 Loss 1.5576\n","Time taken for 1 epoch 387.4286730289459 sec\n","\n","Epoch 10 Batch 0 Loss 1.4667\n","Epoch 10 Batch 100 Loss 1.6096\n","Epoch 10 Batch 200 Loss 1.5680\n","Epoch 10 Batch 300 Loss 1.5889\n","Epoch 10 Loss 1.5485\n","Time taken for 1 epoch 383.52880787849426 sec\n","\n","Epoch 10 Loss 1.5485\n","Time taken for 1 epoch 383.5290343761444 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qSGR56Z050UR","executionInfo":{"status":"ok","timestamp":1603643920339,"user_tz":420,"elapsed":752,"user":{"displayName":"Nicholas Pagliocca","photoUrl":"","userId":"15154366603312876591"}},"outputId":"b26452ca-9d35-4e9e-f19f-bcb94147796a","colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["def evaluate(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  hidden = [tf.zeros((1, units))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  for t in range(max_length_targ):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","    result += targ_lang.index_word[predicted_id] + ' '\n","\n","    if targ_lang.index_word[predicted_id] == '<end>':\n","      return result, sentence\n","\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence\n","\n","def translate(sentence):\n","    result, sentence = evaluate(sentence)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: {}'.format(result))\n","\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","translate(u'llueve en primavera.')\n","translate(u'esta es mi vida.')\n","translate(u'¿todavia estan en casa?')\n","translate(u'trata de averiguarlo.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: <start> llueve en primavera . <end>\n","Predicted translation: i m not a good . <end> \n","Input: <start> esta es mi vida . <end>\n","Predicted translation: i m not a good . <end> \n","Input: <start> ¿ todavia estan en casa ? <end>\n","Predicted translation: i m not a good . <end> \n","Input: <start> trata de averiguarlo . <end>\n","Predicted translation: i m not a good . <end> \n"],"name":"stdout"}]}]}